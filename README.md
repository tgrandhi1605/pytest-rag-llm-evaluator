# pytest-rag-llm-evaluator

This repository provides a Pytest-based framework designed to evaluate Retrieval-Augmented Generation (RAG) based Large Language Models (LLMs). This tool helps developers and researchers validate the performance, accuracy, and reliability of custom LLM architectures using structured tests and industry-standard metrics.

## ğŸ’¡Overview

This framework enables end-to-end evaluation of RAG pipelines, covering:
* 	Retrieval Modules
* 	Augmentation Modules
* 	Generation Modules

It integrates with the RAGAS library, OpenAI, and Langchain to deliver actionable insights on LLM performance.

## âš™ï¸ Tech stack
1. Python
2. Pytest
3. RAGAS Library
4. OpenAI API
5. Langchain

## ğŸ“Š Evaluation Metrics
1. âœ… Context Precision
2. âœ… Context Recall
3. âœ… Faithfulness
4. âœ… Factual Correctness
5. âœ… Response Relevancy
6. âœ… Topic Adherence
7. âœ… Rubrics Score

## ğŸ” Features

1ï¸âƒ£ Multi-Stage Testing Scope

Test your RAG system at each stage â€” retrieval, augmentation, and generation â€” to identify weaknesses early.

3ï¸âƒ£ Multiple Metrics Evaluation

Leverage EvalDataSet to calculate and benchmark LLM responses against ground truth data using multiple metrics.

4ï¸âƒ£ Multi-Conversational Scenarios

Simulate real-world multi-turn conversations to test your LLMâ€™s consistency and contextual understanding.

5ï¸âƒ£ Synthetic Test Data

Generate synthetic question-answer pairs to stress-test your LLMâ€™s capabilities.

6ï¸âƒ£ Test Optimization

All tests are designed with pytest standards, making it easy to extend, automate, and integrate into CI/CD pipelines.

## ğŸ“‚ Repository Structure
````plaintext
pytest-rag-llm-evaluator/
â”œâ”€â”€ tests/                # Pytest test cases
â”œâ”€â”€ data/                 # Synthetic & real datasets
â”œâ”€â”€ metrics/              # Metric definitions and assertions
â”œâ”€â”€ configs/              # Config files for different LLM setups
â”œâ”€â”€ README.md             # Project documentation
â””â”€â”€ requirements.txt      # Dependencies`
````

