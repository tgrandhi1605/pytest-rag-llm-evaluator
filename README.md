# pytest-rag-llm-evaluator

This repository provides a Pytest-based framework designed to evaluate Retrieval-Augmented Generation (RAG) based Large Language Models (LLMs). This tool helps developers and researchers validate the performance, accuracy, and reliability of custom LLM architectures using structured tests and industry-standard metrics.

## ğŸ’¡Overview

This framework enables end-to-end evaluation of [RAG](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/) pipelines, covering:
* 	Retrieval Modules
* 	Augmentation Modules
* 	Generation Modules

It integrates with the RAGAS library, OpenAI, and Langchain to deliver actionable insights on LLM performance.

## âš™ï¸ Tech stack
1. [Python](https://www.python.org/downloads/)
2. [Pytest](https://docs.pytest.org/en/stable/)
3. [RAGAS](https://docs.ragas.io/) Library
4. [OpenAI API](https://platform.openai.com/)
5. [Langchain](https://python.langchain.com/docs/introduction/)

## ğŸ“Š Evaluation Metrics
1. âœ… [Context Precision](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/context_precision/)
2. âœ… [Context Recall](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/context_recall/)
3. âœ… [Faithfulness](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/faithfulness/)
4. âœ… [Factual Correctness](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/factual_correctness/)
5. âœ… [Response Relevancy](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/answer_relevance/)
6. âœ… [Topic Adherence](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/agents/#topic_adherence)
7. âœ… [Rubrics Score](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/general_purpose/#rubrics-based-scoring)

## ğŸ” Features

1ï¸âƒ£ Multi-Stage Testing Scope

Test your RAG system at each stage â€” retrieval, augmentation, and generation â€” to identify weaknesses early.

3ï¸âƒ£ Multiple Metrics Evaluation

Leverage EvalDataSet to calculate and benchmark LLM responses against ground truth data using multiple metrics.

4ï¸âƒ£ Multi-Conversational Scenarios

Simulate real-world multi-turn conversations to test your LLMâ€™s consistency and contextual understanding.

5ï¸âƒ£ Synthetic Test Data

Generate synthetic question-answer pairs to stress-test your LLMâ€™s capabilities.

6ï¸âƒ£ Test Optimization

All tests are designed with pytest standards, making it easy to extend, automate, and integrate into CI/CD pipelines.

## ğŸ“‚ Repository Structure
````plaintext
pytest-rag-llm-evaluator/
â”œâ”€â”€ tests/                # Pytest test cases
â”œâ”€â”€ data/                 # Synthetic & real datasets
â”œâ”€â”€ metrics/              # Metric definitions and assertions
â”œâ”€â”€ configs/              # Config files for different LLM setups
â”œâ”€â”€ README.md             # Project documentation
â””â”€â”€ requirements.txt      # Dependencies`
````

